{"categories":[],"posts":[{"content":"anidb-index v0.2.0 发布，使用内部框架进行了重构。\nREADME:\n 介绍 环境 配置方法 使用方法  介绍 本项目作为项目Project Nichijou中的子项目，是AniDB的数据库标题索引，用于构建番剧数据库。完整内容详见: https://github.com/project-nichijou/intro\n本项目根据内部规范，基于内部框架进行开发。\n本repo只包含：\n 从官方API获取标题XML数据 写入数据库  环境  MySQL 5.7.4 + Python 3.6 + mysql-connector-python click  配置方法 common子项目的配置，详见这里。由于本项目比较简单，所以只需要配置数据库字段即可。\n使用方法 通过CLI调用，下面是说明：\n$ python3 main.py --help Usage: main.py [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. Commands: dellog delete loggings in the database. download download the anidb title index database dump parse download then save the parse and save the result into the...  $ python3 main.py download --help Usage: main.py download [OPTIONS] download the anidb title index database dump Options: --url TEXT use other url to download database instead of the one in the configuration file --ignore_cache whether to ignore cache files --help Show this message and exit.  $ python3 main.py dellog --help Usage: main.py dellog [OPTIONS] delete loggings in the database. Options: --before TEXT delete the loggings which are before the time in the database. default is None, which means delete all. data format: YYYY-MM-DD hh:mm:ss --help Show this message and exit.  ","id":0,"section":"posts","summary":"anidb-index v0.2.0 发布，使用内部框架进行了重构。 README: 介绍 环境 配置方法 使用方法 介绍 本项目作为项目Project Nichijou中的子项目，是AniDB的数据库","tags":null,"title":"Anidb Index v0.2.0","uri":"https://project-nichijou.github.io/2021/07/anidb-index-v0.2.0/","year":"2021"},{"content":"重构版本 v0.3.0 发布。\nREADME:\n 介绍 关于爬虫 环境 配置方法 关于Cookies 使用方法 关于脚本 一些其他奇奇怪怪的点 Change log  介绍 本项目为Project Nichijou中的子项目，是Bangumi 番组计划的爬虫，用于构建番剧数据库。完整内容详见: https://github.com/project-nichijou/intro\n本项目根据内部规范，基于内部框架进行开发。\n关于爬虫 本项目实现了如下Spider:\n bangumi_anime_list: 爬取动画列表, /anime/browser/?sort=title\u0026amp;page=\u0026lt;page\u0026gt; bangumi_anime_api: 爬取API提供的番剧信息 bangumi_anime_scrape: 爬取网页上的番剧信息 /subject/\u0026lt;sid\u0026gt;  所有属性列表 (HTML) 标签 (空格隔开) 种类 (TV, OVA, \u0026hellip;)    因为主项目的性质，故主要精力集中在番剧上面，如果您有其他需要可以自行实现 (欢迎提交PR！)\n环境  MySQL 5.7.4 + Python 3.6 + Scrapy beautifulsoup4 Ubuntu (WSL) click (optional, 用于构建CLI) mysql-connector-python dill  配置方法  common子项目的配置，详见这里 bangumi/config/bangumi_settings中配置本项目的相关字段bangumi (默认情况下无需更改) bangumi/config/scrapy_settings中配置scrapy的相关字段 (默认情况下无需更改)  关于Cookies cookies.json中的cookies在某些情况下需要以下字段 (已经写在template当中了)，否则无法爬取特殊内容，但是我们仍然建议把整个cookies都复制进来:\n{ \u0026quot;chii_auth\u0026quot;: \u0026lt;value\u0026gt;, \u0026quot;chii_sec_id\u0026quot;: \u0026lt;value\u0026gt;, \u0026quot;chii_sid\u0026quot;: \u0026lt;value\u0026gt; }  注意：chii_sid由于会被Bangumi定期替换，所以我们手动实现了cookies变更持久化。当然，即便这样我们在爬取的时候还是有可能失效，毕竟我们只能停留在对机制的猜测阶段。是否开启cookies持久化可以在bangumi/config/bangumi_settings.py中的COOKIES_AUTO_UPDATE当中设置。如果开启，建议复制一份cookies.json保存为cookies.json.backup，因为文件会被复写，备份以便不时之需。\n注意 (怎么又来了)：关于cookies.json的格式：支持list和dict的两种格式。\n上面的是dict，下面的是list举例：\n[ { \u0026quot;domain\u0026quot;: \u0026quot;.bgm.tv\u0026quot;, \u0026quot;expirationDate\u0026quot;: 1627660426.06073, \u0026quot;hostOnly\u0026quot;: false, \u0026quot;httpOnly\u0026quot;: false, \u0026quot;name\u0026quot;: \u0026quot;chii_auth\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/\u0026quot;, \u0026quot;sameSite\u0026quot;: \u0026quot;unspecified\u0026quot;, \u0026quot;secure\u0026quot;: false, \u0026quot;session\u0026quot;: false, \u0026quot;storeId\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;\u0026lt;value\u0026gt;\u0026quot;, \u0026quot;id\u0026quot;: 1 }, // 略... ]  推荐使用list配置cookies.json，不容易抽风。方法：可以使用类似于EditThisCookie的插件进行导出，见下图。\n使用方法 经过考量，不准备使用scrapyd或者写启动服务器之类的功能，这里只提供了可以用于定时执行的脚本以及main.py的CLI工具。我们计划在以后统一实现后端整个工作流的控制管理，不在这里单一实现。目前，可以直接通过以下命令启动爬虫：\nscrapy crawl \u0026lt;spider_name\u0026gt;  \u0026lt;spider_name\u0026gt;即为蜘蛛的文件名，位于bangumi/spider/目录下。\n注意：对于部分蜘蛛，如bangumi_anime，有额外的参数。如果需要传参，请使用如下命令：\nscarpy crawl \u0026lt;spider_name\u0026gt; -a \u0026lt;arg1\u0026gt;=\u0026lt;val1\u0026gt; \u0026lt;arg2\u0026gt;=\u0026lt;val2\u0026gt; ...  比如：\nscrapy crawl bangumi_anime -a fail=off  或者也可以使用CLI命令：\n 主命令 python3 main.py Usage: main.py [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. Commands: crawl start SPIDER crawling using scrapy dellog delete loggings in the database. initdb init bangumi database setcookies set cookies of bangumi    crawl python3 main.py crawl --help Usage: main.py crawl [OPTIONS] SPIDER start SPIDER crawling using scrapy SPIDER: name of the spider to start Options: --fail INTEGER time of retrying for failed items. default is 0, when the value is negative, retrying won't stop unless the table `request_failed` is empty. Note: this parameter is not available for all spiders, only for `bangumi_anime_api`, `bangumi_anime_scrape`. --help Show this message and exit.    dellog python3 main.py dellog --help Usage: main.py dellog [OPTIONS] delete loggings in the database. Options: --before TEXT delete the loggings which are before the time in the database. default is None, which means delete all. data format: YYYY-MM-DD hh:mm:ss --help Show this message and exit.    setcookies python3 main.py setcookies --help Usage: main.py setcookies [OPTIONS] COOKIES set cookies of bangumi COOKIES: dictionary of cookies (converted to str) Options: --help Show this message and exit.      关于脚本 可以发现，在仓库的根目录我们还提供了下面的脚本:\n run.sh  之所以提供脚本其实是因为scrapy没有提供定位到特定目录开始任务的命令行参数选项\u0026hellip;所以我们就手动实现一下咯。\n此脚本自动按顺序启动下面三个蜘蛛:\n bangumi_anime_list bangumi_anime_scrape bangumi_anime_api  失败重试的次数设置为了两次，可以自行调整。\n一些其他奇奇怪怪的点  Bangumi自身数据有很大的可能性出问题，包括但不限于  排行榜数据重复, 看这里, 和这里 官方API返回的数据字段缺失 数据错误等 某些奇怪的字段数据过长 (比如date, duration)   本项目中, parse出的所有结果一律使用yield, 不得使用return, 否则可能会出现无法进入pipelines的情况。原因不明，但是本项目中发生过这样有一个很典型的例子  Change log  v0.1.0 v0.2.0 v0.2.1  ","id":1,"section":"posts","summary":"重构版本 v0.3.0 发布。 README: 介绍 关于爬虫 环境 配置方法 关于Cookies 使用方法 关于脚本 一些其他奇奇怪怪的点 Change log 介绍 本项目为Project Nichijo","tags":null,"title":"Bangumi Spider v0.3.0 发布","uri":"https://project-nichijou.github.io/2021/07/bangumi-spider-v0.3.0/","year":"2021"},{"content":"对之前的bangumi-spider进行了重构，根据之前制定的内部格式对Scrapy进行二次开发，抽象出了这个内部爬虫框架。\nREADME:\n 介绍 结构 使用方法 接口  common.spiders.common_spider.CommonSpider  use_cookies initialize init_normal_datasource init_fail_datasource   common.items.common_item.CommonItem  table primary_keys _url use_fail   关于cache    介绍 本项目是Project Nichijou的一个子项目。根据内部规范实现的基于Scrapy二次开发的爬虫框架。\n结构 common ├── cache │ ├── cache_maker.py # cache 生产 │ └── cache_response.py # 封装过的cache Response ├── config │ ├── settings.py # 配置文件 │ └── settings_template.py # 配置模板 ├── cookies │ ├── cookies.json # cookies │ ├── cookies.json.backup # cookies 备份 │ ├── cookies_io.py # cookies的IO封装 │ └── cookies_template.json # cookies 模板 ├── database │ ├── database.py # 数据库 │ └── database_command.py # 根据 [规范] 封装的 建表命令 ├── items # 根据 [规范] 封装的 Item │ ├── anime_item.py │ ├── anime_name_item.py │ ├── cache_item.py │ ├── common_item.py # Item 自定义父类 │ ├── episode_item.py │ ├── episode_name_item.py │ ├── fail_request_item.py │ └── log_item.py ├── middlewares │ ├── cache_middleware.py # 请求缓存中间件 │ └── cookie_middleware.py # cookies持久化中间件 ├── pipelines │ └── storing_pipeline.py # 储存管道 ├── spiders │ └── common_spider.py # Spider 自定义父类 └── utils ├── ac.py # AC 自动机封装 ├── checker.py # 数据有效性封装 ├── datetime.py # 日期时间格式封装 ├── formatter.py # 格式化工具封装 ├── hash.py # 哈希工具封装 └── logger.py # 日志封装  使用方法  根据下面的template进行配置 (复制到同目录并重命名)  common/cookies/cookies_template.json common/config/settings_template.py   在主项目中配置scrapy的配置文件，重点有如下字段： DOWNLOADER_MIDDLEWARES = { 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': None, 'common.middlewares.cookie_middleware.CommonCookiesMiddleware': 920, 'common.middlewares.cache_middleware.CommonCacheMiddleware': 930, } ITEM_PIPELINES = { 'common.pipelines.storing_pipeline.CommonStoringPipeline': 300, }  `` 注意：上面的配置不是必须的\n  可以考虑如下几种使用方式:\n 子类继承父类，自定义某些字段，覆写值 传参使用。注意：Item只有变量名为_开始的才能够作为属性直接修改，否则需要通过dict的方式。  接口 common.spiders.common_spider.CommonSpider  parent: scrapy.Spider  use_cookies  type: boolean desc: 为True则为此蜘蛛启用cookies组件，为False则不启用。注意：启用的前提是在settings中配置了middlewares  initialize  type: function desc: 初始化spider  init_normal_datasource  type: function desc: 初始化正常情况下的数据源  init_fail_datasource  type: function desc: 初始化失败重试情况下的数据源  common.items.common_item.CommonItem  parent: scrapy.Item  table  type: str desc: 此Item将被保存到的数据表  primary_keys  type: list desc: 存入数据表的primary_keys (主键)，用于update数据，若此项缺失，则会直接覆写  _url  type: str desc: 产生该Item请求的url，用于删除fail记录  use_fail  type: boolean desc: 此Item是否回进行重试，或重试时是否需要删除失败记录  关于cache CommonCacheMiddleware只处理了cache的读取，cache的写入需要在Spider中实现。可以使用common/cache/cache_maker.py当中封装过的函数实现。\n","id":2,"section":"posts","summary":"对之前的bangumi-spider进行了重构，根据之前制定的内部格式对Scrapy进行二次开发，抽象出了这个内部爬虫框架。 README: 介绍 结构 使用方","tags":null,"title":"Common Spider v0.1.0 发布","uri":"https://project-nichijou.github.io/2021/07/common-spider-v0.1.0/","year":"2021"},{"content":"拿Hugo的pure主题搭了一个开发日志的blog，此后将在这里更新开发情况ヾ(≧▽≦*)o\n","id":3,"section":"posts","summary":"拿Hugo的pure主题搭了一个开发日志的blog，此后将在这里更新开发情况ヾ(≧▽≦*)o","tags":null,"title":"Link Start!","uri":"https://project-nichijou.github.io/2021/07/hello/","year":"2021"}],"tags":[]}